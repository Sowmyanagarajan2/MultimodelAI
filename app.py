# -*- coding: utf-8 -*-
"""MultimodelAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B15heJnJdjJ7vz_4nKL4MXEUSlVHpOuW
"""

from transformers import CLIPProcessor, CLIPModel
  #converting images into a compatible representation
from PIL import Image
import requests

#load clip model
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
#load image
# Example image URL
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# Display the loaded image
display(image)
#define candidate texts
texts  = ['A photo of a dog','a photo of a cat']

#preprocess both img and text
inputs = processor(text = texts,images=image,return_tensors='pt',padding=True)
outputs = model(**inputs)
similarity= outputs.logits_per_image.softmax(dim=1).detach().numpy()
print("similarity score",similarity[0])

# Process the image
#inputs = processor(images=image, return_tensors="pt")

# Now you can use the inputs with the model
# For example, to get image features:
# image_features = model.get_image_features(**inputs)
# display(image_features)

# To get text features (you would need to define text inputs first):
# text_inputs = processor(text=["a photo of a cat", "a photo of a dog"], return_tensors="pt")
# text_features = model.get_text_features(**text_inputs)
# display(text_features)

